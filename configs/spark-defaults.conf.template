# ==============================================================================
# Spark Connect Server Configuration
# ==============================================================================
#
# IMPORTANT: Why These Configurations Must Be Set Server-Side
#
# Spark Connect introduced a decoupled client-server architecture in Spark 3.4+
# where the client and Spark driver run in SEPARATE PROCESSES. This architectural
# change has important implications for configuration management:
#
# 1. CLIENT-SERVER SEPARATION:
#    - Client: Thin API that translates DataFrame operations into logical plans
#    - Server: Spark driver that executes queries and manages cluster resources
#    - Communication: gRPC protocol with Protocol Buffers encoding
#
# 2. CONFIGURATION CATEGORIES:
#
#    a) STATIC CONFIGURATIONS (Server-Side Only):
#       - Must be set when the Spark Connect server starts
#       - Cannot be modified by clients after server initialization
#       - Affect core Spark behavior, JVM settings, and cluster setup
#       - Examples: extensions, catalogs, warehouse dir, resource allocation
#       - Attempting to set these from client causes CANNOT_MODIFY_CONFIG errors
#
#    b) RUNTIME CONFIGURATIONS (Client-Side Allowed):
#       - Can be set via spark.conf.set() from client applications
#       - Affect query execution behavior (e.g., shuffle partitions, AQE)
#       - Session-specific and don't impact other connected clients
#
# 3. WHY IMMUTABLE ON CLIENT:
#    - Security: Prevents clients from modifying global cluster settings
#    - Stability: Ensures consistent behavior across all client sessions
#    - Isolation: Multiple clients can't interfere with each other's resources
#    - JVM Constraints: Some configs require JVM restart to take effect
#
# 4. CHECKING MUTABILITY:
#    You can check if a config is mutable: spark.conf.isModifiable("config.name")
#
# References:
# - https://spark.apache.org/docs/latest/spark-connect-overview.html
# - https://spark.apache.org/docs/latest/configuration.html
#
# ==============================================================================

# ------------------------------------------------------------------------------
# Delta Lake Configuration (STATIC - Server-Side Only)
# ------------------------------------------------------------------------------
# These SQL extensions must be loaded when the Spark server starts.
# They initialize Delta Lake support by registering custom SparkSessionExtensions
# and catalog implementations that handle Delta table operations.

spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension,org.apache.sedona.sql.SedonaSqlExtensions
spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog

# Delta Lake settings
spark.databricks.delta.retentionDurationCheck.enabled=false

# Event logging (will be set to user directory by startup script)
spark.eventLog.enabled=true
# spark.eventLog.dir will be set dynamically

# Event log rolling configuration for S3/MinIO
# Since S3 doesn't support append operations, Spark uses a rolling strategy
# to write event logs in chunks that are uploaded as complete files
spark.eventLog.rolling.enabled=true
spark.eventLog.rolling.maxFileSize=10m

# Buffer settings for S3 event logging
spark.eventLog.buffer.kb=100k
spark.eventLog.compress=true
spark.eventLog.compression.codec=zstd

# Warehouse directory - DO NOT SET globally!
# Each database should specify its own LOCATION when created via create_namespace_if_not_exists()
# This allows user warehouses and tenant warehouses to coexist in the same Spark session
# Tables created without a database will fail (which is the intended behavior)

# Hive metastore
spark.sql.catalogImplementation=hive
spark.sql.hive.metastore.version=4.0.0
spark.sql.hive.metastore.jars=path
spark.sql.hive.metastore.jars.path=/usr/local/spark/jars/*
# hive.metastore.uris will be set dynamically from BERDL_HIVE_METASTORE_URI

# MinIO S3 configuration - will be set dynamically by startup script
# spark.hadoop.fs.s3a.endpoint will be set from MINIO_ENDPOINT_URL
# spark.hadoop.fs.s3a.access.key will be set from user's credentials
# spark.hadoop.fs.s3a.secret.key will be set from user's credentials
spark.hadoop.fs.s3a.path.style.access=true
spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem
spark.hadoop.fs.s3a.connection.ssl.enabled=false

# ------------------------------------------------------------------------------
# KBase Authentication Interceptor (STATIC - Server-Side Only)
# ------------------------------------------------------------------------------
# This interceptor validates KBase tokens for Spark Connect requests:
# - Same-pod connections (Localhost, ::1, and Pod IP): Allowed without authentication
# - Remote connections: Require valid KBase token, verified against Auth2 service
#
# Environment variables used by the interceptor:
# - KBASE_AUTH_URL: KBase Auth2 service URL (Required, e.g. https://ci.kbase.us/services/auth/)
# - USER: Pod owner username (Required for validating remote access permissions)
# - BERDL_POD_IP: Pod's own IP address (Required for identifying same-pod connections)
spark.connect.grpc.interceptor.classes=us.kbase.spark.KBaseAuthServerInterceptor
